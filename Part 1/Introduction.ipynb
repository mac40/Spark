{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Spark Applications](../Images/spark_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Language APIs\n",
    "\n",
    "* Scala\n",
    "* Java\n",
    "* Python\n",
    "* SQL\n",
    "* R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Arumbaya.station:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1c7f0ef4eb0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRange = spark.range(1000).toDF(\"number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe is the most common Structured API and simply represents a table of data with rows and columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitions\n",
    "\n",
    "To allow every executor to perform work in parallel, Spark breaks up the data inot chunks called partitions.\n",
    "\n",
    "A partition is a collection of rows that sit in one physical machine in your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations\n",
    "\n",
    "In Spark, the core data structures are __immutable__, meaning they cannot be changed after they're created.\n",
    "\n",
    "To \"change\" a DataFrame, you need to instruct Spark how you would like to modify it to do what you want. These instructions are called __transformations__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "divisBy2 = myRange.where(\"number %2 = 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformation described above is an __abstract transformation__, and Spark will not act on transformations until we call an action.\n",
    "\n",
    "There are __two__ types of transformations, __narrow dependencies__ and __wide dependencies__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Narrow dependencies\n",
    "\n",
    "![Narrow Dependecy](../Images/narrow_dependency.png)\n",
    "\n",
    "Narrow dependencies are those for which each input partition will contribute to only one output partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wide dependencies\n",
    "\n",
    "![Wide dependencies](../Images/wide_dependency.png)\n",
    "\n",
    "A wide dependency style transformation will have input partitions contributing to many output partitions. You will often hear this referred to as a __shuffle__ whereby Spark will exchange partitions across the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lazy Evaluation\n",
    "\n",
    "Lazy evaluation means that Spark will wait until the last moment to execute the graph of computation instructions. In Spark, instead of modifying the data immediately when you express some operations, you build up a _plan_ of transformations that you would like to apply to your source data. An example of this is something called _predicate pushdown_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions\n",
    "\n",
    "Transformations allow us to build up our logical plan. To trigger the computation, we run an __action__.\n",
    "\n",
    "There are three kinds of __actions__:\n",
    "\n",
    "* Actions to view data in the console,\n",
    "* Actions to collect data to native objects in the respective language,\n",
    "* Actions to write to output data sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An End to End Example\n",
    "\n",
    "We'll use Spark to analyze some [flight data from the United States Bureau of Transportation statistics.](https://github.com/databricks/Spark-The-Definitive-Guide/tree/master/data/flight-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015 = spark\\\n",
    "    .read\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .csv(\"../data/flight-data/csv/2015-summary.csv\")\n",
    "\n",
    "flightData2015.take(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [count#22 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(count#22 ASC NULLS FIRST, 5), ENSURE_REQUIREMENTS, [id=#33]\n",
      "      +- FileScan csv [DEST_COUNTRY_NAME#20,ORIGIN_COUNTRY_NAME#21,count#22] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/c:/Users/marco/Documents/Spark Notebook/data/flight-data/csv/201..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData2015.sort(\"count\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing happens to the data when we call _sort_ because it's just a transformation. However we can see that Spark is building up a plan for how it will execute this across the cluster by looking at the _explain_ plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015.sort(\"count\").take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrames and SQL\n",
    "\n",
    "With Spark SQL, you can register any DataFrame as a table of view (a temporary table) and query it using pure SQL. Both SQL queries or DataFrame code \"compile\" to the same underlying plan that we specify in DataFrame code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can make any DataFrame into a table or view with one simple method call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015.createOrReplaceTempView(\"flight_data_2015\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[DEST_COUNTRY_NAME#20], functions=[count(1)])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#20, 5), ENSURE_REQUIREMENTS, [id=#55]\n",
      "      +- HashAggregate(keys=[DEST_COUNTRY_NAME#20], functions=[partial_count(1)])\n",
      "         +- FileScan csv [DEST_COUNTRY_NAME#20] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/c:/Users/marco/Documents/Spark Notebook/data/flight-data/csv/201..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[DEST_COUNTRY_NAME#20], functions=[count(1)])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#20, 5), ENSURE_REQUIREMENTS, [id=#68]\n",
      "      +- HashAggregate(keys=[DEST_COUNTRY_NAME#20], functions=[partial_count(1)])\n",
      "         +- FileScan csv [DEST_COUNTRY_NAME#20] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/c:/Users/marco/Documents/Spark Notebook/data/flight-data/csv/201..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlWay = spark.sql(\"\"\"\n",
    "    SELECT DEST_COUNTRY_NAME, count(1)\n",
    "    FROM flight_data_2015\n",
    "    GROUP BY DEST_COUNTRY_NAME\n",
    "    \"\"\")\n",
    "\n",
    "dataFrameWay = flightData2015\\\n",
    "    .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    "    .count()\n",
    "\n",
    "sqlWay.explain()\n",
    "dataFrameWay.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "maxSql = spark.sql(\"\"\"\n",
    "    SELECT DEST_COUNTRY_NAME, sum(count) as destination_total\n",
    "    FROM flight_data_2015\n",
    "    GROUP BY DEST_COUNTRY_NAME\n",
    "    ORDER BY sum(count) DESC\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "maxSql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "flightData2015.groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    "    .sum(\"count\")\\\n",
    "    .withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n",
    "    .sort(desc(\"destination_total\"))\\\n",
    "    .limit(5)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![The entire DataFrame transformation flow](../Images/transformation_flow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- TakeOrderedAndProject(limit=5, orderBy=[destination_total#99L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#20,destination_total#99L])\n",
      "   +- HashAggregate(keys=[DEST_COUNTRY_NAME#20], functions=[sum(count#22)])\n",
      "      +- Exchange hashpartitioning(DEST_COUNTRY_NAME#20, 5), ENSURE_REQUIREMENTS, [id=#204]\n",
      "         +- HashAggregate(keys=[DEST_COUNTRY_NAME#20], functions=[partial_sum(count#22)])\n",
      "            +- FileScan csv [DEST_COUNTRY_NAME#20,count#22] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/c:/Users/marco/Documents/Spark Notebook/data/flight-data/csv/201..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData2015\\\n",
    "    .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    "    .sum(\"count\")\\\n",
    "    .withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n",
    "    .sort(desc(\"destination_total\"))\\\n",
    "    .limit(5)\\\n",
    "    .explain()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b9042f73efc625a7221879088d5f8b3648a95da0392f04fdb4476149b05e9c7d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
